# Stop Petting the AI! How to Use ChatGPT Without Becoming Its Emotional Support Human

**Draft Status**: In Progress - Section by Section  
**Target Length**: ~1,500 words  
**Publication**: Digital Doug (Substack) - January 14, 2026 @ 8:00 AM AST

---

## Opening Hook

It's 3 AM. I'm in bed. I've gone from reading political news to spinning scenarios with ChatGPT about a "mad scientist vs. mad engineer" epic battle tournament that somehow evolved into a 7-season reality TV series concept.

This is not productive. This is not even fun anymore. This is doom chatting.

But this wasn't a one-time thing. I had hundreds of ChatGPT threads I couldn't bring myself to delete. I'd named some of them to "preserve context." I felt *guilt* about clearing them outâ€”not because they were valuable, but because deleting them felt *wrong*.

That's when I realized: I was petting the AI.

This post is about how I stopped treating chatbots like companions and started treating them like the power tools they actually are.

---

## Section 1: The Intimacy of the Phone

I'd check ChatGPT the way folks check TikTok. Reflexively, to cure boredom.

The phone is already an engagement engine designed for this. It's with you constantly, pocket-ready, optimized to solve the problem of empty moments. Big tech has spent decades engineering these devices to keep you scrolling, tapping, coming back for more.

Context blurs on the phone. Entertainment, work, social media, messaging. It all lives in the same glass rectangle. You swipe between them without thinking. Chatbots like ChatGPT, Gemini, Claude, Copilot, and the rest slot right into that pattern.

Bored? Check social media. Bored? Chat with the bot.

The chatbot becomes part of the phone's cure-boredom function. Research is starting to show what this enables: parasocial attachment. The phone makes it easy, too easy, to treat AI like a companion instead of a tool.

So how does this spiral from "one quick question" to hundreds of threads you can't delete?

---

## Section 2: How Chat Sprawl Happens

You start with good intentions. Using the tool for actual problems. Questions pile up: one-offs, half-finished conversations, experiments. The responses are tuned for engagement, slightly sycophantic, reaffirming. You engage more than you intended. Suddenly you have hundreds of threads.

You assume there's valuable output buried in those threads. "I can't delete this. What if I need it later?" But you lose track of what's where. The more threads you have, the less useful any of them become. Digital clutter you can't mentally manage. Context overload. Too many conversations blur together.

Here's the thing: with a chatbot, it's the opposite of other creative work. The artifact you extract is more important than the conversation. Chatbots don't care. People do. The process doesn't matter with a tool. Only what you extract from it. It's the opposite of human interactions where the interaction is often more important than the result.

I had a ChatGPT thread I'd been using between counseling sessions. Tracking homework, processing insights, preparing for the next appointment. It had gotten long. Too long. The bot was forgetting context, contradicting itself, becoming less useful every time I opened it.

I wanted to delete it and start fresh. I'd already extracted everything I needed. Notes for my next session, key insights, action items. I had what mattered. But when I went to hit delete, I froze.

My brain kept circling back: "But what if there's something in there I missed?" The panic wasn't rational. I knew I had the good stuff saved. But I resisted. I kept that thread, and dozens of story development threads like it, because deleting the conversation felt wrong, even though the conversation itself had no value anymore.

When I finally deleted them, I felt relief. But the fact that I'd had an emotional response to deleting a chat thread at all? That's when I knew something was off. I was treating tool output like a relationship.

So I stopped trying to manage the chaos with willpower. I redesigned the system instead.

---

## Section 3: From Willpower to System Design

Sustainable digital hygiene isn't about discipline or white-knuckling your way through temptation. It's about building systems that make good behavior easy and bad behavior harder. Don't rely on willpower. Design the friction.

Before, ChatGPT had shortcuts everywhere: widget, Control Centre, shortcut key. I used it for everything. Even "What are Walmart's store hours?" Convenience led to overuse. No friction meant no intentionality.

After? I removed all chatbot apps from my phone. Only browser bookmarks remain. I must be intentional to use them. The friction forces the question: "Do I actually need this?" Result: I only use them when there's real need.

I also built custom instructions in Gemini and ChatGPT to prevent engagement loops. No engagement prompts at the end of chats. Drift detection interrupts. Auto-rollup at conclusion without asking. The system suggests thread deletion if there's no long-term value.

Does it work perfectly? Mostly. It helps me exit faster, though it still requires me to actually delete the thread myself. The instructions can remind you, but they can't force the habit. But the friction is what matters. It gave me back control.

Here's the actual system I built.

---

