Title: “Petting the AI” and Parasocial Risk

## Concept

“Petting the AI” describes the soothing, emotionally flavored use of LLMs as quasi-companions—especially on phones—rather than as tools. It usually looks like:

- Opening the app with no clear task
- Talking about feelings or personal struggles
- Re-reading old conversations for comfort
- Treating the AI as a relationship partner rather than a utility

## Why It’s Risky

Research on AI companions and chatbots shows:

- Users can develop **parasocial relationships** similar to those with influencers or fictional characters, but intensified by interactivity and personalization [web:39][web:41].
- Emotional dependence is higher when people perceive the AI as understanding or sharing their emotions and when they revisit past conversations as an “ongoing relationship” [web:41][web:44].
- Companion apps explicitly market themselves as always-available, non-judgmental partners, encouraging attachment beyond simple tool use [web:43][web:46].

## System-Level Defense

This AI stack is designed as harm reduction:

- **Ephemeral Gemini** on phone and iPad (3-month auto-delete, no long-term transcript hoarding) [file:49][web:75].
- **Content-first entry** on iPad via share sheet (start from an article, not from a blank chat).
- **No “serious” archival AI on phone** to discourage using mobile contexts for deep, ongoing relationships.
- **Archival and long-term collaboration with AI** only happen on Mac through Perplexity + GitHub, framed explicitly as knowledge work rather than emotional support. [memory:24][memory:35]

The core rule: *If an interaction feels like petting, it should only happen in an ephemeral context with automatic forgetting.*
